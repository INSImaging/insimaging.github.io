---
---


% Year 2024 ---------------------------------------------------------------

@article{zhu2023federated,
  title={Federated Primal-Dual Fixed Point Algorithm},
  author={Zhu, Ya-Nan and Liang, Jingwei and Zhang, Xiaoqun},
  journal={SIAM Journal on Mathematics of Data Science},
  year={2024},
  abbr={Journal},
  bibtex_show={true},
  html={https://arxiv.org/abs/2305.13604},
  abstract={Federated learning (FL) is a distributed learning paradigm that allows several clients to learn a global model without sharing their private data. In this paper, we generalize a primal dual fixed point (PDFP) \cite{PDFP} method to federated learning setting and propose an algorithm called Federated PDFP (FPDFP) for solving composite optimization problems. In addition, a quantization scheme is applied to reduce the communication overhead during the learning process. An $O(1/k)$ convergence rate (where is the communication round) of the proposed FPDFP is provided. Numerical experiments, including graph-guided logistic regression, 3D Computed Tomography (CT) reconstruction are considered to evaluate the proposed algorithm.},
  cate_primary={optimization},
  cate_secondary={learning},
  selected={true},
}


@article{ehrhardt2024guide,
  title={A Guide to Stochastic Optimisation for Large-Scale Inverse Problems},
  author={Ehrhardt, Matthias J. and Kereta, Zeljko and Liang, Jingwei and Tang, Junqi},
  journal={arXiv preprint arXiv:2406.06342},
  year={2024},
  abbr={Preprint},
  bibtex_show={true},
  html={https://arxiv.org/abs/2406.06342},
  abstract={Stochastic optimisation algorithms are the de facto standard for machine learning with large amounts of data. Handling only a subset of available data in each optimisation step dramatically reduces the per-iteration computational costs, while still ensuring significant progress towards the solution. Driven by the need to solve large-scale optimisation problems as efficiently as possible, the last decade has witnessed an explosion of research in this area. Leveraging the parallels between machine learning and inverse problems has allowed harnessing the power of this research wave for solving inverse problems. In this survey, we provide a comprehensive account of the state-of-the-art in stochastic optimisation from the viewpoint of inverse problems. We present algorithms with diverse modalities of problem randomisation and discuss the roles of variance reduction, acceleration, higher-order methods, and other algorithmic modifications, and compare theoretical results with practical behaviour. We focus on the potential and the challenges for stochastic optimisation that are unique to inverse imaging problems and are not commonly encountered in machine learning. We conclude the survey with illustrative examples from imaging problems to examine the advantages and disadvantages that this new generation of algorithms bring to the field of inverse problems.},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={true},
}

@article{jiang2024scalable,
  title={A Scalable Sphere-Constrained Magnitude-Sparse SAR Imaging},
  author={Jiang, Ming and Qu, Jiaxuan and Ding, Jinshan and Liang, Jingwei},
  journal={Journal of Nonlinear \& Variational Analysis},
  volume={8},
  number={3},
  year={2024},
  abbr={Journal},
  bibtex_show={true},
  html={https://jnva.biemdas.com/issues/JNVA2024-3-2.pdf},
  abstract={The classical synthetic aperture radar (SAR) imaging techniques based on matched filters are limited by data bandwidth, resulting in limited imaging performance with side lobes and speckles present. To address the high-resolution SAR imaging problem, sparse reconstruction has been extensively investigated. However, the state-of-the-art sparse recovery methods seldom consider the complex-valued reflectivity of the scene and only recover an approximated real-valued scene instead. Furthermore, iterative schemes associated with the sparse recovery methods demand a high computational cost, which limits the practical applications of these methods. In this paper, we establish a sphere-constrained magnitude-sparsity SAR imaging model, aiming at enhancing the SAR imaging quality with high efficiency. We propose a non-convex non-smooth optimization method, which can be accelerated by stochastic average gradient acceleration to be scalable with large-scale problems. Numerical experiments are conducted with point-target and extended-target simulations. On the one hand, the point-target simulation showcases the superiority of our proposed method over the classical methods in terms of resolution. On the other hand, the extended-target simulation with random phases is considered to be in line with the practical scenario, and the results demonstrate that our method outperforms the classical SAR imaging methods and sparse recovery without phase prior in terms of PSNR. Meanwhile, owing to the stochastic acceleration, our method is faster than the existing sparse recovery methods by orders of magnitude.},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
}

% Year 2023 ---------------------------------------------------------------

@article{jiang2023distance,
  title={A Distance Function Based Cascaded Neural Network for Accurate Polyps Segmentation and Classification},
  author={Jiang, Yuanhong and Liang, Jingwei and Xiong, Weiqi and Li, Qiwei and Zhang, Yijue and Chen, Tao and Zhang, Xiaoqun},
  journal={Inverse Problems and Imaging},
  volume={17},
  number={3},
  pages={646--659},
  year={2023},
  publisher={Inverse Problems and Imaging},
  abbr={Journal},
  bibtex_show={true},
  html={https://www.aimsciences.org/article/doi/10.3934/ipi.2022069},
  abstract={In clinical practice, accurate locating and measuring the size of polyps in general is a difficult task. In this work, based on the position constraint between the primary organ and polyps boundary, we propose a U-Net based cascaded neural network for the joint segmentation of the organ of interest and the polyps. The position constraint is enforced by considering a narrow-band distance function and complimentary dice function in the loss function. Through a series of comparisons and ablation study, we evaluate the performance of our proposed cascaded network architecture and the additional loss functions using an in-house dataset for gallbladder polyps segmentation and classification. Numerical results indicate that the proposed method achieves a significant improvement over conventional U-Net, U-Net++, etc.. Lastly, the pathological type classification based on the segmented polys shows 30\% higher accuracy compared to those conventional ResNet based results.},
  cate_primary={learning},  
  cate_secondary={none},
  selected={false},
}

@inproceedings{zhou2023robust,
  title={Robust Graph Representation Learning for Local Corruption Recovery},
  author={Zhou, Bingxin and Jiang, Yuanhong and Wang, Yuguang and Liang, Jingwei and Gao, Junbin and Pan, Shirui and Zhang, Xiaoqun},
  booktitle={Proceedings of the ACM Web Conference},
  pages={438--448},
  year={2023},
  abbr={Conference},
  bibtex_show={true},
  html={https://dl.acm.org/doi/abs/10.1145/3543507.3583399},
  abstract={The performance of graph representation learning is affected by the quality of graph input. While existing research usually pursues a globally smoothed graph embedding, we believe the rarely observed anomalies are as well harmful to an accurate prediction. This work establishes a graph learning scheme that automatically detects (locally) corrupted feature attributes and recovers robust embedding for prediction tasks. The detection operation leverages a graph autoencoder, which does not make any assumptions about the distribution of the local corruptions. It pinpoints the positions of the anomalous node attributes in an unbiased mask matrix, where robust estimations are recovered with sparsity promoting regularizer. The optimizer approaches a new embedding that is sparse in the framelet domain and conditionally close to input observations. Extensive experiments are provided to validate our proposed model can recover a robust graph representation from black-box poisoning and achieve excellent performance.},
  cate_primary={learning},  
  cate_secondary={none},
  selected={false},
}

% Year 2022 ---------------------------------------------------------------

@article{lewis2022partial,
  title={Partial Smoothness and Constant Rank},
  author={Lewis, Adrian S. and Liang, Jingwei and Tian, Tonghua},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={1},
  pages={276--291},
  year={2022},
  publisher={SIAM},
  abbr={Journal},
  bibtex_show={true},
  html={https://epubs.siam.org/doi/abs/10.1137/19M1237909},
  abstract={In optimization, the notion of a partly smooth objective function is powerful for applications in algorithmic convergence and postoptimality analysis, and yet is complex to define. A shift in focus to the first-order optimality conditions reduces the concept to a simple constant-rank condition. In this view, partial smoothness extends to more general variational systems, encompassing in particular the saddlepoint operators underlying popular primal-dual splitting algorithms. For a broad class of semi-algebraic generalized equations, partial smoothness holds generically.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={true},
}

@article{liang2022variable,
  title={Variable Screening for Sparse Online Regression},
  author={Liang, Jingwei and Poon, Clarice},
  journal={Journal of Computational and Graphical Statistics},
  pages={1--19},
  year={2022},
  publisher={Taylor \& Francis},
  abbr={Journal},
  bibtex_show={true},
  html={https://www.tandfonline.com/doi/full/10.1080/10618600.2022.2099872},
  abstract={Sparsity-promoting regularizers are widely used to impose low-complexity structure (e.g., l1-norm for sparsity) to the regression coefficients of supervised learning. In the realm of deterministic optimization, the sequence generated by iterative algorithms (such as proximal gradient descent) exhibit “finite activity identification” property, that is, they can identify the low-complexity structure of the solution in a finite number of iterations. However, many online algorithms (such as proximal stochastic gradient descent) do not have this property owing to the vanishing step-size and nonvanishing variance. In this article, by combining with a screening rule, we show how to eliminate useless features of the iterates generated by online algorithms, and thereby enforce finite sparsity identification. One advantage of our scheme is that when combined with any convergent online algorithm, sparsity properties imposed by the regularizer can be exploited to improve computational efficiency. Numerically, significant acceleration can be obtained.},
  cate_primary={optimization},
  cate_secondary={learning},
  selected={false},
}

@article{driggs2022biased,
  title={On Biased Stochastic Gradient Estimation},
  author={Driggs, Derek and Liang, Jingwei and Sch{\"o}nlieb, Carola-Bibiane},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={24},
  pages={1--43},
  year={2022},
  abbr={Journal},
  bibtex_show={true},
  html={https://www.jmlr.org/papers/v23/20-316.html},
  abstract={We present a uniform analysis of biased stochastic gradient methods for minimizing convex, strongly convex, and non-convex composite objectives, and identify settings where bias is useful in stochastic gradient estimation. The framework we present allows us to extend proximal support to biased algorithms, including SAG and SARAH, for the first time in the convex setting. We also use our framework to develop a new algorithm, Stochastic Average Recursive GradiEnt (SARGE), that achieves the oracle complexity lower-bound for non-convex, finite-sum objectives and requires strictly fewer calls to a stochastic gradient oracle per iteration than SVRG and SARAH. We support our theoretical results with numerical experiments that demonstrate the benefits of certain biased gradient estimators. },
  cate_primary={optimization},
  cate_secondary={learning},
  selected={true},
}

@article{wei2022tfpnp,
  title={TFPNP: Tuning-free Plug-and-Play Proximal Algorithms with Applications to Inverse Imaging Problems},
  author={Wei, Kaixuan and Aviles-Rivero, Angelica and Liang, Jingwei and Fu, Ying and Huang, Hua and Sch{\"o}nlieb, Carola-Bibiane},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={16},
  pages={1--48},
  year={2022},
  abbr={Journal},
  bibtex_show={true},
  html={https://www.jmlr.org/papers/v23/20-1297.html},
  abstract={Plug-and-Play (PnP) is a non-convex optimization framework that combines proximal algorithms, for example, the alternating direction method of multipliers (ADMM), with advanced denoising priors. Over the past few years, great empirical success has been obtained by PnP algorithms, especially for the ones that integrate deep learning-based denoisers. However, a key problem of PnP approaches is the need for manual parameter tweaking which is essential to obtain high-quality results across the high discrepancy in imaging conditions and varying scene content. In this work, we present a class of tuning-free PnP proximal algorithms that can determine parameters such as denoising strength, termination time, and other optimization-specific parameters automatically. A core part of our approach is a policy network for automated parameter search which can be effectively learned via a mixture of model-free and model-based deep reinforcement learning strategies. We demonstrate, through rigorous numerical and visual experiments, that the learned policy can customize parameters to different settings, and is often more efficient and effective than existing handcrafted criteria. Moreover, we discuss several practical considerations of PnP denoisers, which together with our learned policy yield state-of-the-art results. This advanced performance is prevalent on both linear and nonlinear exemplar inverse imaging problems, and in particular shows promising results on compressed sensing MRI, sparse-view CT, single-photon imaging, and phase retrieval. },
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
}

@article{liang2022improving,
  title={Improving “Fast Iterative Shrinkage-Thresholding Algorithm”: Faster, Smarter, and Greedier},
  author={Liang, Jingwei and Luo, Tao and Schönlieb, Carola-Bibiane},
  journal={SIAM Journal on Scientific Computing},
  volume={44},
  number={3},
  pages={A1069--A1091},
  year={2022},
  publisher={SIAM},
  abbr={Journal},
  bibtex_show={true},
  html={https://epubs.siam.org/doi/abs/10.1137/21M1395685},
  abstract={The “fast iterative shrinkage-thresholding algorithm,” a.k.a. FISTA, is one of the most well known first-order optimization scheme in the literature, as it achieves the worst-case $O(1/k^2)$ optimal convergence rate for objective function value. However, despite such an optimal theoretical convergence rate, in practice the (local) oscillatory behavior of FISTA often damps its efficiency. Over the past years, various efforts have been made in the literature to improve the practical performance of FISTA, such as monotone FISTA, restarting FISTA, and backtracking strategies. In this paper, we propose a simple yet effective modification to the original FISTA scheme which has two advantages: It allows us to (1) prove the convergence of generated sequence and (2) design a so-called lazy-start strategy which can be up to an order faster than the original scheme. Moreover, we propose novel adaptive and greedy strategies which further improve the practical performance. The advantages of the proposed schemes are tested through problems arising from inverse problems, machine learning, and signal/image processing.},
  code={https://github.com/jliang993/Faster-FISTA},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={true},
}

% Year 2021 ---------------------------------------------------------------

@article{bian2021stochastic,
  title={A Stochastic Alternating Direction Method of Multipliers for Non-smooth and Non-convex Optimization},
  author={Bian, Fengmiao and Liang, Jingwei and Zhang, Xiaoqun},
  journal={Inverse Problems},
  volume={37},
  year={2021},
  publisher={IOP Publishing},
  abbr={Journal},
  bibtex_show={true},
  html={https://iopscience.iop.org/article/10.1088/1361-6420/ac0966/meta},
  abstract={Alternating direction method of multipliers (ADMM) is a popular first-order method owing to its simplicity and efficiency. However, similar to other proximal splitting methods, the performance of ADMM degrades significantly when the scale of optimization problems to solve becomes large. In this paper, we consider combining ADMM with a class of variance-reduced stochastic gradient estimators for solving large-scale non-convex and non-smooth optimization problems. Global convergence of the generated sequence is established under the additional assumption that the object function satisfies Kurdyka-Łojasiewicz property. Numerical experiments on graph-guided fused lasso and computed tomography are presented to demonstrate the performance of the proposed methods.},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={false},
}

@article{driggs2021stochastic,
  title={A Stochastic Proximal Alternating Minimization for Nonsmooth and Nonconvex Optimization},
  author={Driggs, Derek and Tang, Junqi and Liang, Jingwei and Davies, Mike and Schönlieb, Carola-Bibiane},
  journal={SIAM Journal on Imaging Sciences},
  volume={14},
  number={4},
  pages={1932--1970},
  year={2021},
  publisher={Society for Industrial and Applied Mathematics},
  abbr={Journal},
  bibtex_show={true},
  html={https://epubs.siam.org/doi/abs/10.1137/20M1387213},
  abstract={In this work, we introduce a novel stochastic proximal alternating linearized minimization algorithm [J. Bolte, S. Sabach, and M. Teboulle, Math. Program., 146 (2014), pp. 459--494] for solving a class of nonsmooth and nonconvex optimization problems. Large-scale imaging problems are becoming increasingly prevalent due to the advances in data acquisition and computational capabilities. Motivated by the success of stochastic optimization methods, we propose a stochastic variant of proximal alternating linearized minimization. We provide global convergence guarantees, demonstrating that our proposed method with variance-reduced stochastic gradient estimators, such as SAGA [A. Defazio, F. Bach, and S. Lacoste-Julien, Advances in Neural Information Processing Systems, 2014, pp. 1646--1654] and SARAH [L. M. Nguyen, J. Liu, K. Scheinberg, and M. Takáĉ, Proceedings of the 34th International Conference on Machine Learning, PMLR 70, 2017, pp. 2613--2621], achieves state-of-the-art oracle complexities. We also demonstrate the efficacy of our algorithm via several numerical examples including sparse nonnegative matrix factorization, sparse principal component analysis, and blind image-deconvolution.},
  code={https://gitlab.com/jesuslovesbilly/SPRING1.0},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={true},
}

@article{dutta2021adaptive,
  title={An Adaptive Rank Continuation Algorithm for General Weighted Low-rank Recovery},
  author={Dutta, Aritra and Liang, Jingwei and Li, Xin},
  journal={arXiv preprint arXiv:2101.00749},
  year={2021},
  abbr={Preprint},
  bibtex_show={true},
  html={https://arxiv.org/abs/2101.00749},
  abstract={This paper is devoted to proposing a general weighted low-rank recovery model and designing a fast SVD-free computational scheme to solve it. First, our generic weighted low-rank recovery model unifies several existing approaches in the literature.~Moreover, our model readily extends to the non-convex setting. Algorithm-wise, most first-order proximal algorithms in the literature for low-rank recoveries require computing singular value decomposition (SVD). As SVD does not scale appropriately with the dimension of the matrices, these algorithms become slower when the problem size becomes larger. By incorporating the variational formulation of the nuclear norm into the sub-problem of proximal gradient descent, we avoid computing SVD, which results in significant speed-up. Moreover, our algorithm preserves the rank identification property of nuclear norm [33] which further allows us to design a rank continuation scheme that asymptotically achieves the minimal iteration complexity. Numerical experiments on both toy examples and real-world problems, including structure from motion (SfM) and photometric stereo, background estimation, and matrix completion, demonstrate the superiority of our proposed algorithm.},
  code={https://github.com/jliang993/ProGrAMMe},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}

% Year 2020 ---------------------------------------------------------------

@inproceedings{wei2020tuning,
  title={Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems},
  author={Wei, Kaixuan and Aviles-Rivero, Angelica and Liang, Jingwei and Fu, Ying and Sch{\"o}nlieb, Carola-Bibiane and Huang, Hua},
  booktitle={International Conference on Machine Learning <b>(ICML)</b>},
  pages={10158--10169},
  year={2020},
  organization={PMLR},
  abbr={Conference},
  bibtex_show={true},
  html={https://proceedings.mlr.press/v119/wei20b.html},
  abstract={ Plug-and-play (PnP) is a non-convex framework that combines ADMM or other proximal algorithms with advanced denoiser priors. Recently, PnP has achieved great empirical success, especially with the integration of deep learning-based denoisers. However, a key problem of PnP based approaches is that they require manual parameter tweaking. It is necessary to obtain high-quality results across the high discrepancy in terms of imaging conditions and varying scene content. In this work, we present a tuning-free PnP proximal algorithm, which can automatically determine the internal parameters including the penalty parameter, the denoising strength and the terminal time. A key part of our approach is to develop a policy network for automatic search of parameters, which can be effectively learned via mixed model-free and model-based deep reinforcement learning. We demonstrate, through numerical and visual experiments, that the learned policy can customize different parameters for different states, and often more efficient and effective than existing handcrafted criteria. Moreover, we discuss the practical considerations of the plugged denoisers, which together with our learned policy yield state-of-the-art results. This is prevalent on both linear and nonlinear exemplary inverse imaging problems, and in particular, we show promising results on Compressed Sensing MRI and phase retrieval. },
  award={This paper received the **Outstanding Paper Award** from [ICML2020](https://icml.cc/Conferences/2020/Awards).},
  award_name={Outstanding Paper Award},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={true},
}

@article{poon2020geometry,
  title={Geometry of First-Order Methods and Adaptive Acceleration},
  author={Poon, Clarice and Liang, Jingwei},
  journal={arXiv preprint arXiv:2003.03910},
  year={2020},
  abbr={Preprint},
  bibtex_show={true},
  html={https://arxiv.org/abs/2003.03910},
  abstract={First-order operator splitting methods are ubiquitous among many fields through science and engineering, such as inverse problems, signal/image processing, statistics, data science and machine learning, to name a few. In this paper, we study a geometric property of first-order methods when applying to solve non-smooth optimization problems. With the tool of "partial smoothness", we design a framework to analyze the trajectory of the fixed-point sequence generated by first-order methods and show that locally, the fixed-point sequence settles onto a regular trajectory such as a straight line or a spiral. Based on this finding, we discuss the limitation of current widely used "inertial acceleration" technique, and propose a trajectory following adaptive acceleration algorithm. Global convergence is established for the proposed acceleration scheme based on the perturbation of fixed-point iteration. Locally, we first build connections between the acceleration scheme and the well-studied "vector extrapolation technique" in the field of numerical analysis, and then discuss local acceleration guarantees of the proposed acceleration scheme. Moreover, our result provides a geometric interpretation of these vector extrapolation techniques. Numerical experiments on various first-order methods are provided to demonstrate the advantage of the proposed adaptive acceleration scheme.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}

@article{tovey2020fun,
  title={The Fun is Finite: Douglas-Rachford and Sudoku Puzzle--Finite Termination and Local Linear Convergence},
  author={Tovey, Robert and Liang, Jingwei},
  journal={Journal of Applied and Numerical Optimization},
  year={2020},
  abbr={Journal},
  bibtex_show={true},
  html={https://arxiv.org/abs/2009.04018},
  abstract={In recent years, the Douglas-Rachford splitting method has been shown to be effective at solving many non-convex optimization problems. In this paper we present a local convergence analysis for non-convex feasibility problems and show that both finite termination and local linear convergence are obtained. For a generalization of the Sudoku puzzle, we prove that the local linear rate of convergence of Douglas-Rachford is exactly $$\frac{\sqrt{5}}{5}$$ and independent of puzzle size. For the $s$-queens problem we prove that Douglas-Rachford converges after a finite number of iterations. Numerical results on solving Sudoku puzzles and s-queens puzzles are provided to support our theoretical findings.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}

@article{dutta2020best,
  title={Best Pair Formulation \& Accelerated Scheme for Non-convex Principal Component Pursuit},
  author={Dutta, Aritra and Hanzely, Filip and Liang, Jingwei and Richt{\'a}rik, Peter},
  journal={IEEE Transactions on Signal Processing},
  year={2020},
  publisher={IEEE},
  abbr={Journal},
  bibtex_show={true},
  html={https://ieeexplore.ieee.org/abstract/document/9145595},
  abstract={Given two disjoint sets, the best pair problem aims to find a point in one set and another point in the other set with minimal distance between them. In this paper, we formulate the classical robust principal component analysis (RPCA) problem as a best pair problem and design an accelerated proximal gradient algorithm to solve it. We prove that the method enjoys global convergence with a local linear rate. Our extensive numerical experiments on both real and synthetic data sets suggest that our proposed algorithm outperforms relevant baseline algorithms in the literature.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}


% Year 2019 ---------------------------------------------------------------

@inproceedings{poon2019trajectory,
  title={Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration},
  author={Poon, Clarice and Liang, Jingwei},
  booktitle={Advances in Neural Information Processing Systems <b>(NeurIPS)</b>},
  pages={7355--7363},
  year={2019},
  abbr={Conference},
  bibtex_show={true},
  html={https://proceedings.neurips.cc/paper/2019/hash/587524833eaf98eb779a387e33768c6a-Abstract.html},
  abstract={The alternating direction method of multipliers (ADMM) is one of the most widely used first-order optimisation methods in the literature owing to its simplicity, flexibility and efficiency. Over the years, numerous efforts are made to improve the performance of the method, such as the inertial technique. By studying the geometric properties of ADMM, we discuss the limitations of current inertial accelerated ADMM and then present and analyze an adaptive acceleration scheme for the method. Numerical experiments on problems arising from image processing, statistics and machine learning demonstrate the advantages of the proposed acceleration approach.},
  code={https://github.com/jliang993/A3DMM},
  highlights={Oral presentation},
  highlights_detail={This paper was presented as an **oral presentation** at [NeurIPS2019](https://neurips.cc/Conferences/2019).},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={true},
}

@article{molinari2019convergence,
  title={Convergence Rates of Forward--Douglas--Rachford Splitting Method},
  author={Molinari, Cesare and Liang, Jingwei and Fadili, Jalal},
  journal={Journal of Optimization Theory and Applications},
  pages={1--34},
  year={2019},
  publisher={Springer US},
  abbr={Journal},
  bibtex_show={true},
  html={https://link.springer.com/article/10.1007/s10957-019-01524-9},
  abstract={Over the past decades, operator splitting methods have become ubiquitous for non-smooth optimization owing to their simplicity and efficiency. In this paper, we consider the Forward-Douglas-Rachford splitting method and study both global and local convergence rates of this method. For the global rate, we establish a sublinear convergence rate in terms of a Bregman divergence suitably designed for the objective function. Moreover, when specializing to the Forward-Backward splitting, we prove a stronger convergence rate result for the objective function value. Then locally, based on the assumption that the non-smooth part of the optimization problem is partly smooth, we establish local linear convergence of the method. More precisely, we show that the sequence generated by Forward-Douglas-Rachford first (i) identifies a smooth manifold in a finite number of iteration and then (ii) enters a local linear convergence regime, which is for instance characterized in terms of the structure of the underlying active smooth manifold. To exemplify the usefulness of the obtained result, we consider several concrete numerical experiments arising from applicative fields including, for instance, signal/image processing, inverse problems and machine learning.},
  code={https://github.com/jliang993/Rate-FDR},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}

% Year 2018 ---------------------------------------------------------------

@inproceedings{poon2018local,
  title={Local Convergence Properties of SAGA/Prox-SVRG and Acceleration},
  author={Poon, Clarice and Liang, Jingwei and Sch{\"o}nlieb, Carola-Bibiane},
  booktitle={International Conference on Machine Learning <b>(ICML)</b>},
  pages={4124--4132},
  year={2018},
  abbr={Conference},
  bibtex_show={true},
  html={https://proceedings.mlr.press/v80/poon18a.html},
  abstract={In this paper, we present a local convergence anal- ysis for a class of stochastic optimisation meth- ods: the proximal variance reduced stochastic gradient methods, and mainly focus on SAGA (Defazio et al., 2014) and Prox-SVRG (Xiao & Zhang, 2014). Under the assumption that the non-smooth component of the optimisation prob- lem is partly smooth relative to a smooth mani- fold, we present a unified framework for the local convergence analysis of SAGA/Prox-SVRG: (i) the sequences generated by the methods are able to identify the smooth manifold in a finite num- ber of iterations; (ii) then the sequence enters a local linear convergence regime. Furthermore, we discuss various possibilities for accelerating these algorithms, including adapting to better lo- cal parameters, and applying higher-order deter- ministic/stochastic optimisation methods which can achieve super-linear convergence. Several concrete examples arising from machine learning are considered to demonstrate the obtained result.},
  code={https://github.com/jliang993/Local-VRSGD},
  cate_primary={optimization},
  cate_secondary={learning},
  selected={true},
}

@article{liang2018local,
  title={Local Linear Convergence Analysis of Primal--Dual Splitting Methods},
  author={Liang, Jingwei and Fadili, Jalal and Peyr{\'e}, Gabriel},
  journal={Optimization},
  number={6},
  pages={821--853},
  year={2018},
  publisher={Taylor \& Francis},
  abbr={Journal},
  bibtex_show={true},
  html={https://www.tandfonline.com/doi/abs/10.1080/02331934.2018.1426584},
  abstract={In this paper, we study the local linear convergence properties of a versatile class of Primal-Dual splitting methods for minimizing composite non-smooth convex optimization problems. Under the assumption that the non-smooth components of the problem are partly smooth relative to smooth manifolds, we present a unified local convergence analysis framework for these methods. More precisely, in our framework, we first show that (i) the sequences generated by Primal-Dual splitting methods identify a pair of primal and dual smooth manifolds in a finite number of iterations, and then (ii) enter a local linear convergence regime, which is characterized based on the structure of the underlying active smooth manifolds. We also show how our results for Primal-Dual splitting can be specialized to cover existing ones on Forward-Backward splitting and Douglas-Rachford splitting/ADMM (alternating direction methods of multipliers). Moreover, based on these obtained local convergence analysis result, several practical acceleration techniques are discussed. To exemplify the usefulness of the obtained result, we consider several concrete numerical experiments arising from fields including signal/image processing, inverse problems and machine learning. The demonstration not only verifies the local linear convergence behaviour of Primal-Dual splitting methods, but also the insights on how to accelerate them in practice.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}


% Year 2017 ---------------------------------------------------------------


@article{liang2017activity,
  title={Activity Identification and Local Linear Convergence of Forward--Backward-type Methods},
  author={Liang, Jingwei and Fadili, Jalal and Peyr{\'e}, Gabriel},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={1},
  pages={408--437},
  year={2017},
  publisher={SIAM},
  abbr={Journal},
  bibtex_show={true},
  html={https://epubs.siam.org/doi/abs/10.1137/16M106340X},
  abstract={In this paper, we consider a class of Forward--Backward (FB) splitting methods that includes several variants (e.g., inertial schemes, FISTA) for minimizing the sum of two proper convex and lower semicontinuous functions, one of which has a Lipschitz continuous gradient, and the other is partly smooth relative to a smooth active manifold $M$. We propose a unified framework, under which we show that this class of FB-type algorithms (i) correctly identifies the active manifold in a finite number of iterations (finite activity identification), and (ii) then enters a local linear convergence regime, which we characterize precisely in terms of the structure of the underlying active manifold. We also establish and explain why FISTA (with convergent sequences) locally oscillates and can be locally slower than FB. These results may have numerous applications including in signal/image processing, sparse recovery and machine learning. Indeed, the obtained results explain the typical behaviour that has been observed numerically for many problems in these fields such as the Lasso, the group Lasso, the fused Lasso and the nuclear norm minimization to name only a few.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={true},
}

@article{liang2017local,
  title={Local Convergence Properties of Douglas--Rachford and Alternating Direction Method of Multipliers},
  author={Liang, Jingwei and Fadili, Jalal and Peyr{\'e}, Gabriel},
  journal={Journal of Optimization Theory and Applications},
  volume={172},
  number={3},
  pages={874--913},
  year={2017},
  publisher={Plenum Press},
  abbr={Journal},
  bibtex_show={true},
  html={https://link.springer.com/article/10.1007/s10957-017-1061-z},
  abstract={The Douglas-Rachford and alternating direction method of multipliers are two proximal splitting algorithms designed to minimize the sum of two proper lower semi-continuous convex functions whose proximity operators are easy to compute. The goal of this work is to understand the local linear convergence behaviour of Douglas-Rachford (resp. alternating direction method of multipliers) when the involved functions (resp. their Legendre-Fenchel conjugates) are moreover partly smooth. More precisely, when the two functions (resp. their conjugates) are partly smooth relative to their respective smooth submanifolds, we show that Douglas-Rachford (resp. alternating direction method of multipliers) (i) identifies these manifolds in finite time; (ii) enters a local linear convergence regime. When both functions are locally polyhedral, we show that the optimal convergence radius is given in terms of the cosine of the Friedrichs angle between the tangent spaces of the identified submanifolds. Under polyhedrality of both functions, we also provide conditions sufficient for finite convergence. The obtained results are illustrated by several concrete examples and supported by numerical experiments.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}

% Year 2016 ---------------------------------------------------------------

@phdthesis{liang2016convergence,
  title={Convergence Rates of First-Order Operator Splitting Methods},
  author={Liang, Jingwei},
  year={2016},
  school={Normandie Universit{\'e}; GREYC CNRS UMR 6072},
  abbr={Thesis},
  bibtex_show={true},
  html={https://hal.science/tel-01388978/},
  abstract={ This manuscript is concerned with convergence analysis of first-order operator splitting methods that are ubiquitous in modern non-smooth optimization. It consists of three main theoretical advances on this class of methods, namely global convergence rates, novel operator splitting schemes and local linear convergence. First, we propose global (sub-linear) and local (linear) convergence rates for the inexact Krasnosel’skii-Mann iteration built from non-expansive operators, and its application to a variety of monotone operator splitting schemes. Then we design two novel multi-step inertial operator splitting algorithms, both in the convex and non-convex settings, and prove their global convergence. Finally, building on the key concept of partial smoothness, we present a unified and sharp local linear convergence analysis for the class of first-order proximal splitting methods for optimization. We show that for all these algorithms, under appropriate non-degeneracy conditions, the iterates generated by each of these methods will (i) identify the involved partial smooth manifolds in finite time, and then (ii) enter a local linear convergence regime. The linear convergence rates are characterized precisely based on the structure of the optimization problems, that of the proximal splitting scheme, and the geometry of the identified active manifolds. Our theoretical findings are systematically illustrated on applications arising from inverse problems, signal/image processing and machine learning. },
  cate_primary={optimization},
  cate_secondary={none},
  selected={true},
}

@article{liang2016convergence,
  title={Convergence Rates with Inexact Non-expansive Operators},
  author={Liang, Jingwei and Fadili, Jalal and Peyr{\'e}, Gabriel},
  journal={Mathematical Programming},
  volume={159},
  number={1},
  pages={403--434},
  year={2016},
  publisher={Springer Berlin Heidelberg},
  abbr={Journal},
  bibtex_show={true},
  html={https://link.springer.com/article/10.1007/s10107-015-0964-4},
  abstract={In this paper, we present a convergence rate analysis for the inexact Krasnosel'skiĭ-Mann iteration built from non-expansive operators. The presented results include two main parts: we first establish the global pointwise and ergodic iteration-complexity bounds; then, under a metric sub-regularity assumption, we establish a local linear convergence for the distance of the iterates to the set of fixed points. The obtained results can be applied to analyze the convergence rate of various monotone operator splitting methods in the literature, including the Forward-Backward splitting, the Generalized Forward-Backward, the Douglas-Rachford splitting, alternating direction method of multipliers and Primal-Dual splitting methods. For these methods, we also develop easily verifiable termination criteria for finding an approximate solution, which can be seen as a generalization of the termination criterion for the classical gradient descent method. We finally develop a parallel analysis for the non-stationary Krasnosel'skiĭ-Mann iteration.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={true},
}

@inproceedings{liang2016multi,
  title={A Multi-step Inertial Forward-Backward Splitting Method for Non-convex Optimization},
  author={Liang, Jingwei and Fadili, Jalal and Peyr{\'e}, Gabriel},
  booktitle={Advances in Neural Information Processing Systems <b>(NeurIPS)</b>},
  pages={4035--4043},
  year={2016},
  abbr={Conference},
  bibtex_show={true},
  html={https://proceedings.neurips.cc/paper/2016/hash/ea6b2efbdd4255a9f1b3bbc6399b58f4-Abstract.html},
  abstract={In this paper, we propose a multi-step inertial Forward--Backward splitting algorithm for minimizing the sum of two non-necessarily convex functions, one of which is proper lower semi-continuous while the other is differentiable with a Lipschitz continuous gradient. We first prove global convergence of the scheme with the help of the Kurdyka-Łojasiewicz property. Then, when the non-smooth part is also partly smooth relative to a smooth submanifold, we establish finite identification of the latter and provide sharp local linear convergence analysis. The proposed method is illustrated on a few problems arising from statistics and machine learning.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={true},
}

% Year 2015 ---------------------------------------------------------------

@inproceedings{liang2015activity,
  title={Activity Identification and Local Linear Convergence of Douglas--Rachford/ADMM under Partial Smoothness},
  author={Liang, Jingwei and Fadili, Jalal and Peyr{\'e}, Gabriel and Luke, Russell},
  booktitle={International Conference on Scale Space and Variational Methods in Computer Vision  <b>(SSVM)</b>},
  pages={642--653},
  year={2015},
  organization={Springer, Cham},
  abbr={Conference},
  bibtex_show={true},
  html={https://link.springer.com/chapter/10.1007/978-3-319-18461-6_51},
  abstract={Convex optimization has become ubiquitous in most quantitative disciplines of science, including variational image processing. Proximal splitting algorithms are becoming popular to solve such structured convex optimization problems. Within this class of algorithms, Douglas-Rachford (DR) and ADMM are designed to minimize the sum of two proper lower semi-continuous convex functions whose proximity operators are easy to compute. The goal of this work is to understand the local convergence behaviour of DR (resp. ADMM) when the involved functions (resp. their Legendre-Fenchel conjugates) are moreover partly smooth. More precisely, when both of the two functions (resp. their conjugates) are partly smooth relative to their respective manifolds, we show that DR (resp. ADMM) identifies these manifolds in finite time. Moreover, when these manifolds are affine or linear, we prove that DR/ADMM is locally linearly convergent with a rate in terms of the cosine of the Friedrichs angle between the tangent spaces of the identified manifolds. This is illustrated by several concrete examples and supported by numerical experiments.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}

@article{liang2015retinex,
  title={Retinex by Higher Order Total Variation L1 Decomposition},
  author={Liang, Jingwei and Zhang, Xiaoqun},
  journal={Journal of Mathematical Imaging and Vision},
  volume={52},
  number={3},
  pages={345--355},
  year={2015},
  publisher={Springer},
  abbr={Journal},
  bibtex_show={true},
  html={https://link.springer.com/article/10.1007/s10851-015-0568-x},
  abstract={In this paper, we propose a reflectance and illumination decomposition model for the Retinex problem via high-order total variation and $L^1$ decomposition. Based on the observation that illumination varies smoother than reflectance, we propose a convex variational model which can effectively decompose the gradient field of images into salient edges and relatively smoother illumination field through the first- and second-order total variation regularizations. The proposed model can be efficiently solved by a primal-dual splitting method. Numerical experiments on both grayscale and color images show the strength of the proposed model with applications to Retinex illusions, medical image bias field removal, and color image shadow correction.},
  cate_primary={imaging},
  cate_secondary={none},
  selected={true},
}

% Year 2014 ---------------------------------------------------------------

@article{liang2014local,
  title={Local Linear Convergence of Forward--Backward under Partial Smoothness},
  author={Liang, Jingwei and Fadili, Jalal and Peyr{\'e}, Gabriel},
  journal={Advances in Neural Information Processing Systems <b>(NeurIPS)</b>},
  volume={27},
  pages={1970--1978},
  year={2014},
  abbr={Conference},
  bibtex_show={true},
  html={https://proceedings.neurips.cc/paper/2014/hash/522a9ae9a99880d39e5daec35375e999-Abstract.html},
  abstract={In this paper, we consider the Forward--Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $M$. We propose a generic framework in which we show that the Forward--Backward (i) correctly identifies the active manifold M in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
}

@article{liang2014seismic,
  title={Seismic Data Restoration via Data-driven Tight Frame},
  author={Liang, Jingwei and Ma, Jianwei and Zhang, Xiaoqun},
  journal={Geophysics},
  volume={79},
  number={3},
  pages={65--74},
  year={2014},
  publisher={Society of Exploration Geophysicists},
  abbr={Journal},
  bibtex_show={true},
  html={https://library.seg.org/doi/abs/10.1190/geo2013-0252.1},
  abstract={ Restoration/interpolation of missing traces plays a crucial role in the seismic data processing pipeline. Efficient restoration methods have been proposed based on sparse signal representation in a transform domain such as Fourier, wavelet, curvelet, and shearlet transforms. Most existing methods are based on transforms with a fixed basis. We considered an adaptive sparse transform for restoration of data with complex structures. In particular, we evaluated a data-driven tight-frame-based sparse regularization method for seismic data restoration. The main idea of the data-driven tight frame (TF) is to adaptively learn a set of framelet filters from the currently interpolated data, under which the data can be more sparsely represented; hence, the sparsity-promoting l1-norm (SPL1) minimization methods can produce better restoration results by using the learned filters. A split inexact Uzawa algorithm, which can be viewed as a generalization of the alternating direction of multiplier method (ADMM), was applied to solve the presented SPL1 model. Numerical tests were performed on synthetic and real seismic data for restoration of randomly missing traces over a regular data grid. Our experiments showed that our proposed method obtains the state-of-the-art restoration results in comparison with the traditional Fourier-based projection onto convex sets, the tight-frame-based method, and the recent shearlet regularization ADMM method.},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
}

% Year 2013 ---------------------------------------------------------------


@article{liang2013wavelet,
  title={Wavelet Frame Based Color Image Demosaicing},
  author={Liang, Jingwei and Li, Jia and Shen, Zuowei and Zhang, Xiaoqun},
  journal={Inverse Problems and Imaging},
  volume={7},
  number={3},
  pages={777--794},
  year={2013},
  abbr={Journal},
  bibtex_show={true},
  html={https://www.aimsciences.org/article/doi/10.3934/ipi.2013.7.777},
  abstract={Color image demosaicing consists in recovering full resolution color information from color-filter-array (CFA) samples with 66.7% amount of missing data. Most of the existing color demosaicing methods [14, 25, 16, 2, 26] are based on interpolation from inter-channel correlation and local geometry, which are not robust to highly saturated color images with small geometric features. In this paper, we introduce wavelet frame based methods by using a sparse wavelet [8, 22, 9, 23] approximation of individual color channels and color differences that recovers both geometric features and color information. The proposed models can be efficiently solved by Bregmanized operator splitting algorithm [27]. Numerical simulations of two datasets: McM and Kodak PhotoCD, show that our method outperforms other existing methods in terms of PSNR and visual quality.},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
}








@article{ding2021deep,
  title={Deep Learning with Adaptive Hyper-parameters for Low-Dose CT Image Reconstruction},
  author={Ding, Qiaoqiao and Nan, Yuesong and Gao, Hao and Ji, Hui},
  journal={IEEE Transactions on Computational Imaging},
  year={2021},
  abbr={Journal},
  selected={true},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={true},
}


@article{bin2022nomogram,
  title={Nomogram based on Clinical and Radiomics Data for Predicting Radiation-induced Temporal Lobe Injury in Patients with Non-metastatic Stage T4 Nasopharyngeal Carcinoma},
  author={Bin, Xiang and Zhu, Chao hua and Tang, Yanyan and Li, Renyuan and Ding, Qiaoqiao and al., et.},
  year={2022},
  publisher={Clinical Oncology},
  abbr={Journal},
  selected={true},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{wang2023prognostic,
  title={The prognostic value of splenic abnormalities in pretreatment 18F-FDG PET/CT in patients with complete response diffuse large B-cell lymphoma},
  author={Wang, S and Ju, H and Bai, Y and Wang, L and Ding, Q and Li, P and Jiang, X and Lin, X},
  journal={Clinical Radiology},
  volume={78},
  number={5},
  pages={375--380},
  year={2023},
  publisher={WB Saunders},
  abbr={Journal},
  selected={true},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{jiang2023automatic,
  title={Automatic recognition of white blood cell images with memory efficient superpixel metric GNN: SMGNN},
  author={Jiang, Yuanhong and Shen, Yiqing and Wang, Yu Guang and Ding, Qiaoqiao},
  year={2023},
  publisher={Preprints},
  abbr={Preprint},
  selected={true},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{cai2024nf,
  title={NF-ULA: Langevin Monte Carlo with normalizing flow prior for imaging inverse problems},
  author={Cai, Ziruo and Tang, Junqi and Mukherjee, Subhadip and Li, Jinglai and Sch{\"o}nlieb, Carola-Bibiane and Zhang, Xiaoqun},
  journal={SIAM Journal on Imaging Sciences},
  volume={17},
  number={2},
  pages={10--1137},
  year={2024},
  abbr={Journal},
  selected={true},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{xiong2024convergence,
  title={On the Convergence of Continuous and Discrete Unbalanced Optimal Transport Models for 1-Wasserstein Distance},
  author={Xiong, Zhe and Li, Lei and Zhu, Ya-Nan and Zhang, Xiaoqun},
  journal={SIAM Journal on Numerical Analysis},
  volume={62},
  number={2},
  pages={749--774},
  year={2024},
  publisher={SIAM},
  abbr={Journal},
  selected={true},
  cate_primary={learning},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{jiang2024vision,
  title={Vision graph u-net: Geometric learning enhanced encoder for medical image segmentation and restoration},
  author={Jiang, Yuanhong and Ding, Qiaoqiao and Wang, Yu Guang and Lio, Pietro and Zhang, Xiaoqun},
  journal={Inverse Problems and Imaging},
  volume={18},
  number={3},
  pages={672--689},
  year={2024},
  publisher={Inverse Problems and Imaging},
  abbr={Journal},
  cate_primary={learning},
  cate_secondary={imaging},
  selected={false},
  bibtex_show={true},
  selected={false},
}




@article{cai2023approximate,
  title={Approximate primal-dual fixed-point based langevin algorithms for non-smooth convex potentials},
  author={Cai, Ziruo and Li, Jinglai and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:2304.04544},
  year={2023},
  abbr={Preprint},
  cate_primary={optimization},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhu2023orthogonal,
  title={An orthogonal matching pursuit optimization method for solving minimum-monitor-unit problems: applications to proton IMPT, ARC and FLASH},
  author={Zhu, Ya-Nan and Zhang, Xiaoqun and Lin, Yuting and Lominska, Chris and Gao, Hao},
  journal={Medical physics},
  volume={50},
  number={8},
  pages={4710--4720},
  year={2023},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={false},
  bibtex_show={true},
  selected={false},
}





@article{xiong2023convergence,
  title={On the convergence of continuous and discrete unbalanced optimal transport models},
  author={Xiong, Zhe and Li, Lei and Zhu, Ya-Nan and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:2303.17267},
  year={2023},
  abbr={Preprint},
  cate_primary={learning},
  cate_secondary={optimization},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{bian2022stochastic,
  title={A stochastic three-block splitting algorithm and its application to quantized deep neural networks},
  author={Bian, Fengmiao and Liu, Ren and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:2204.11065},
  year={2022},
  abbr={Preprint},
  cate_primary={optimization},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{zhang2021variable,
  title={Variable metric extrapolation proximal iterative hard thresholding method for $\ell_0$ minimization problem.},
  author={Zhang, Xue and Zhang, Xiaoqun},
  journal={CoRR},
  year={2021},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{liu2020semi,
  title={Semi-Implicit Back Propagation},
  author={Liu, Ren and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:2002.03516},
  year={2020},
  abbr={Preprint},
  cate_primary={learning},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}




@article{ding2018image,
  title={Image-domain multimaterial decomposition for dual-energy CT based on prior information of material images},
  author={Ding, Qiaoqiao and Niu, Tianye and Zhang, Xiaoqun and Long, Yong},
  journal={Medical physics},
  volume={45},
  number={8},
  pages={3614--3626},
  year={2018},
  publisher={Wiley Online Library},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}




@article{zhang2010bregmanized,
  title={Bregmanized nonlocal regularization for deconvolution and sparse reconstruction},
  author={Zhang, Xiaoqun and Burger, Martin and Bresson, Xavier and Osher, Stanley},
  journal={SIAM Journal on Imaging Sciences},
  volume={3},
  number={3},
  pages={253--276},
  year={2010},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={optimization},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{esser2010general,
  title={A General Framework for a Class of First Order Primal-Dual Algorithms for Convex Optimization in Imaging Science},
  author={Esser, Ernie and Zhang, Xiaoqun and Chan, Tony},
  journal={SIAM Journal on Imaging Sciences},
  volume={3},
  number={4},
  pages={1015--1046},
  year={2010},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{zhang2011unified,
  title={A unified primal-dual algorithm framework based on Bregman iteration},
  author={Zhang, Xiaoqun and Burger, Martin and Osher, Stanley},
  journal={Journal of Scientific Computing},
  volume={46},
  pages={20--46},
  year={2011},
  publisher={Springer US},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={false},    award={百篇最具影响国际学术论文全部产生自2011年引用次数高于学科均线的论文，从SCI收录的我国第一作者论文中选取，论文学术影响的主要文献计量指标为论 文创新性（是否获得重大基金和项目支持）、发表论文的期刊水平（期刊的主要指标所在学科位置）、是否处于研究前沿和是否属于研究热点（即年被引次数）、合著论文中我国作者的主导性（以我为主的国际合作情况）、论文的文献类型（只计Article和Review类型）、论文的参考文献情况（与该学科国际平均水平的比较）和论文的国际知名度（是否发表于世界著名期刊）。这些遴选指标的选取是为了保证了论文的领先型和相对优势。},
  award_name={2011年全国百篇最有影响国际论文},
  bibtex_show={true},
  selected={false},
}

@article{lu2009source,
  title={Source reconstruction for spectrally-resolved bioluminescence tomography with sparse a priori information},
  author={Lu, Yujie and Zhang, Xiaoqun and Douraghy, Ali and Stout, David and Tian, Jie and Chan, Tony F and Chatziioannou, Arion F},
  journal={Optics express},
  volume={17},
  number={10},
  pages={8062--8080},
  year={2009},
  publisher={Optica Publishing Group},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{lou2010image,
  title={Image recovery via nonlocal operators},
  author={Lou, Yifei and Zhang, Xiaoqun and Osher, Stanley and Bertozzi, Andrea},
  journal={Journal of Scientific Computing},
  volume={42},
  number={2},
  pages={185--197},
  year={2010},
  publisher={Springer US},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhang2005total,
  title={Total variation based Fourier reconstruction and regularization for computer tomography},
  author={Zhang, Xiao-Qun and Froment, Jacques},
  booktitle={IEEE Nuclear Science Symposium Conference Record, 2005},
  volume={4},
  pages={2332--2336},
  year={2005},
  organization={IEEE},
  abbr={Conference},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhang2010wavelet,
  title={Wavelet inpainting by nonlocal total variation},
  author={Zhang, Xiaoqun and Chan, Tony F},
  journal={Inverse problems and Imaging},
  volume={4},
  number={1},
  pages={191--210},
  year={2010},
  publisher={American Institute of Mathematical Sciences},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhang2005constrained,
  title={Constrained total variation minimization and application in computerized tomography},
  author={Zhang, Xiao-Qun and Froment, Jacques},
  booktitle={International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition},
  pages={456--472},
  year={2005},
  organization={Springer Berlin Heidelberg Berlin, Heidelberg},
  abbr={Conference},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhang2012novel,
  title={A Novel Sparsity Reconstruction Method from Poisson Data for 3D Bioluminescence Tomography},
  author={Zhang, Xiaoqun and Lu, Yujie and Chan, Tony},
  journal={Journal of Scientific Computing},
  pages={1--17},
  year={2012},
  publisher={Springer},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{chen2013primal,
  title={A primal--dual fixed point algorithm for convex separable minimization with applications to image restoration},
  author={Chen, Peijun and Huang, Jianguo and Zhang, Xiaoqun},
  journal={Inverse Problems},
  volume={29},
  number={2},
  pages={33},
  year={2013},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={false},
  bibtex_show={true},
  selected={true},
}


@article{tai2013wavelet,
  title={Wavelet Frame Based Multiphase Image Segmentation},
  author={Tai, Cheng and Zhang, Xiaoqun and Shen, Zuowei},
  journal={SIAM Journal on Imaging Sciences},
  volume={6},
  number={4},
  pages={2521--2546},
  year={2013},
  publisher={SIAM},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{chang2014domain,
  title={Domain decomposition methods for nonlocal total variation image restoration},
  author={Chang, Huibin and Zhang, Xiaoqun and Tai, Xue-Cheng and Yang, Danping},
  journal={Journal of Scientific Computing},
  volume={60},
  pages={79--100},
  year={2014},
  publisher={Springer US},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{esser2014nonlocal,
  title={Nonlocal patch-based image inpainting through minimization of a sparsity promoting nonconvex functional},
  author={Esser, ERNIE and Zhang, XIAOQUN},
  journal={Dept. Math., Univ. California Irvine, Irvine, CA, USA, Tech. Rep},
  volume={3},
  pages={13},
  year={2014},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{moller2016fast,
  title={Fast Sparse Reconstruction:Greedy Inverse Scale Space Flows},
  author={Moller, Michael and Zhang, Xiaoqun},
  journal={Math. Comp.},
  volume={85},
  pages={179--208},
  year={2016},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}


@article{ding2015dynamic,
  title={Dynamic SPECT reconstruction from few projections: a sparsity enforced matrix factorization approach},
  author={Ding, Qiaoqiao and Zan, Yunlong and Huang, Qiu and Zhang, Xiaoqun},
  journal={Inverse Problems},
  volume={31},
  number={2},
  pages={025004},
  year={2015},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{yu2015interpolation,
  title={Interpolation and denoising of high-dimensional seismic data by learning a tight frame},
  author={Yu, Siwei and Ma, Jianwei and Zhang, Xiaoqun and Sacchi, Mauricio D},
  journal={Geophysics},
  volume={80},
  number={5},
  pages={V119--V132},
  year={2015},
  publisher={Society of Exploration Geophysicists},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{li2015reweighted,
  title={A reweighted l2 method for image restoration with Poisson and mixed Poisson-Gaussian noise},
  author={Li, Jia and Shen, Zuowei and Yin, Rujie and Zhang, Xiaoqun},
  journal={Inverse Problems and Imaging},
  volume={9},
  number={3},
  pages={875--894},
  year={2015},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{hou2016pansharpening,
  title={Pansharpening image fusion using cross-channel correlation: A framelet-based approach},
  author={Hou, Likun and Zhang, Xiaoqun},
  journal={Journal of Mathematical Imaging and Vision},
  volume={55},
  pages={36--49},
  year={2016},
  publisher={Springer US},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhang2015note,
  title={A note on the complexity of proximal iterative hard thresholding algorithm},
  author={Zhang, Xue and Zhang, Xiao-Qun},
  journal={Journal of the Operations Research Society of China},
  volume={3},
  number={4},
  pages={459--473},
  year={2015},
  publisher={Operations Research Society of China Beijing},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{hu2016multi,
  title={Multi-scale features extraction from baseline structure MRI for MCI patient classification and AD early diagnosis},
  author={Hu, Kun and Wang, Yijue and Chen, Kewei and Hou, Likun and Zhang, Xiaoqun},
  journal={Neurocomputing},
  volume={175},
  pages={132--145},
  year={2016},
  publisher={Elsevier},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{chen2016primal,
  title={A primal-dual fixed point algorithm for multi-block convex minimization},
  author={Chen, Peijun and Huang, Jianguo and Zhang, Xiaoqun},
  journal={Journal of Computational Mathematics},
  pages={723--738},
  year={2016},
  publisher={Chinese Academy of Mathematices and Systems Science (AMSS) Chinese Academy},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{choi2016limited,
  title={Limited tomography reconstruction via tight frame and sinogram extrapolation},
  author={Choi, Jae Kyu and Dong, Bin and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:1602.07049},
  year={2016},
  abbr={Preprint},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{hou2016two,
  title={A two-stage low rank approach for calibrationless dynamic parallel magnetic resonance image reconstruction},
  author={Hou, Likun and Gao, Hao and Zhang, Xiaoqun},
  journal={Journal of Scientific Computing},
  volume={69},
  pages={1014--1032},
  year={2016},
  publisher={Springer US},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{burger2016simultaneous,
  title={Simultaneous reconstruction and segmentation for dynamic SPECT imaging},
  author={Burger, Martin and Rossmanith, Carolin and Zhang, Xiaoqun},
  journal={Inverse Problems},
  volume={32},
  number={10},
  pages={104002},
  year={2016},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{liu2016wavelet,
  title={A wavelet frame method with shape prior for ultrasound video segmentation},
  author={Liu, Jiulong and Zhang, Xiaoqun and Dong, Bin and Shen, Zuowei and Gu, Lixu},
  journal={SIAM Journal on Imaging Sciences},
  volume={9},
  number={2},
  pages={495--519},
  year={2016},
  publisher={Society for Industrial and Applied Mathematics},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{zhang2017custom,
  title={CUSTOM: A calibration region recovery approach for highly subsampled dynamic parallel magnetic resonance imaging},
  author={Zhang, Xue and Hou, Likun and Gao, Hao and Zhang, Xiaoqun},
  journal={Journal of Mathematical Imaging and Vision},
  volume={57},
  pages={366--380},
  year={2017},
  publisher={Springer US},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{ding2016modeling,
  title={Modeling Mixed Poisson-Gaussian Noise in Statistical Image Reconstruction for X-Ray CT},
  author={Ding, Qiaoqiao and Long, Yong and Zhang, Xiaoqun and Fessler, Jeffrey A},
  journal={Arbor},
  volume={1001},
  pages={48109},
  year={2016},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{liu2016ticmr,
  title={TICMR: Total image constrained material reconstruction via nonlocal total variation regularization for spectral CT},
  author={Liu, Jiulong and Ding, Huanjun and Molloi, Sabee and Zhang, Xiaoqun and Gao, Hao},
  journal={IEEE transactions on medical imaging},
  volume={35},
  number={12},
  pages={2578--2586},
  year={2016},
  publisher={IEEE},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{choi2018pet,
  title={PET-MRI joint reconstruction by joint sparsity based tight frame regularization},
  author={Choi, Jae Kyu and Bao, Chenglong and Zhang, Xiaoqun},
  journal={SIAM Journal on Imaging Sciences},
  volume={11},
  number={2},
  pages={1179--1204},
  year={2018},
  publisher={Society for Industrial and Applied Mathematics},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{zhang2004regularity,
  title={The regularity of random graph directed self-similar sets},
  author={Zhang, Xiaoqun and Liu, Yanyan},
  journal={Acta Mathematica Scientia},
  volume={24},
  number={3},
  pages={485--492},
  year={2004},
  publisher={No longer published by Elsevier},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{bao2016image,
  title={Image restoration by minimizing zero norm of wavelet frame coefficients},
  author={Bao, Chenglong and Dong, Bin and Hou, Likun and Shen, Zuowei and Zhang, Xiaoqun and Zhang, Xue},
  journal={Inverse problems},
  volume={32},
  number={11},
  pages={115004},
  year={2016},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{ding2018statistical,
  title={Statistical image reconstruction using mixed Poisson-Gaussian noise model for X-ray CT},
  author={Ding, Qiaoqiao and Long, Yong and Zhang, Xiaoqun and Fessler, Jeffrey A},
  journal={arXiv preprint arXiv:1801.09533},
  year={2018},
  abbr={Preprint},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{ding2017dynamic,
  title={Dynamic SPECT reconstruction with temporal edge correlation},
  author={Ding, Qiaoqiao and Burger, Martin and Zhang, Xiaoqun},
  journal={Inverse Problems},
  volume={34},
  number={1},
  pages={014005},
  year={2017},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}


@article{zhang2018pet,
  title={PET-MRI joint reconstruction with common edge weighted total variation regularization},
  author={Zhang, Ying and Zhang, Xiaoqun},
  journal={Inverse Problems},
  volume={34},
  number={6},
  pages={065006},
  year={2018},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{lv2019nltg,
  title={NLTG priors in medical image: Nonlocal TV-Gaussian (NLTG) prior for Bayesian inverse problems with applications to limited CT reconstruction},
  author={Lv, Didi and Zhou, Qingping and Choi, Jae Kyu and Li, Jinglai and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:1901.00262},
  year={2019},
  abbr={Preprint},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhou2020bayesian,
  title={Bayesian inference and uncertainty quantification for medical image reconstruction with Poisson data},
  author={Zhou, Qingping and Yu, Tengchao and Zhang, Xiaoqun and Li, Jinglai},
  journal={SIAM Journal on Imaging Sciences},
  volume={13},
  number={1},
  pages={29--52},
  year={2020},
  publisher={Society for Industrial and Applied Mathematics},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{goldstein2016operator,
  title={Operator splitting methods in compressive sensing and sparse approximation},
  author={Goldstein, Tom and Zhang, Xiaoqun},
  journal={Splitting Methods in Communication, Imaging, Science, and Engineering},
  pages={301--343},
  year={2016},
  publisher={Springer International Publishing},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{choi2020edge,
  title={An edge driven wavelet frame model for image restoration},
  author={Choi, Jae Kyu and Dong, Bin and Zhang, Xiaoqun},
  journal={Applied and Computational Harmonic Analysis},
  volume={48},
  number={3},
  pages={993--1029},
  year={2020},
  publisher={Academic Press},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}


@article{liu2018image,
  title={Image reconstruction by splitting deep learning regularization from iterative inversion},
  author={Liu, Jiulong and Kuang, Tao and Zhang, Xiaoqun},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I},
  pages={224--231},
  year={2018},
  organization={Springer International Publishing},
  abbr={Conference},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhang2019new,
  title={A new proximal iterative hard thresholding method with extrapolation for l0 minimization},
  author={Zhang, Xue and Zhang, Xiaoqun},
  journal={Journal of Scientific Computing},
  volume={79},
  number={2},
  pages={809--826},
  year={2019},
  publisher={Springer US New York},
  abbr={Journal},
  cate_primary=optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{chen2020computer,
  title={Computer-aided diagnosis of gallbladder polyps based on high resolution ultrasonography},
  author={Chen, Tao and Tu, Shaoxiong and Wang, Haolu and Liu, Xuesong and Li, Fenghua and Jin, Wang and Liang, Xiaowen and Zhang, Xiaoqun and Wang, Jian},
  journal={Computer methods and programs in biomedicine},
  volume={185},
  pages={105118},
  year={2020},
  publisher={Elsevier},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}


@article{zhu2020stochastic,
  title={Stochastic primal dual fixed point method for composite optimization},
  author={Zhu, Ya-Nan and Zhang, Xiaoqun},
  journal={Journal of Scientific Computing},
  volume={84},
  number={1},
  pages={16},
  year={2020},
  publisher={Springer US New York},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{jiang2019noise,
  title={Noise suppression in image-domain multi-material decomposition for dual-energy CT},
  author={Jiang, Yangkang and Xue, Yi and Lyu, Qihui and Xu, Lei and Luo, Chen and Yang, Pengfei and Yang, Chunlin and Wang, Jing and Hu, Xi and Zhang, Xiaoqun and others},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={67},
  number={2},
  pages={523--535},
  year={2019},
  publisher={IEEE},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{lv2020nonlocal,
  title={Nonlocal TV-Gaussian prior for Bayesian inverse problems with applications to limited CT reconstruction.},
  author={Lv, Didi and Zhou, Qingping and Choi, Jae Kyu and Li, Jinglai and Zhang, Xiaoqun},
  journal={Inverse Problems \& Imaging},
  volume={14},
  number={1},
  year={2020},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{chen2016primal,
  title={A primal-dual fixed point algorithm for minimization of the sum of three convex separable functions},
  author={Chen, Peijun and Huang, Jianguo and Zhang, Xiaoqun},
  journal={Fixed Point Theory and Applications},
  volume={2016},
  pages={1--18},
  year={2016},
  publisher={Springer International Publishing},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{ding2020low,
  title={Low-dose CT with deep learning regularization via proximal forward--backward splitting},
  author={Ding, Qiaoqiao and Chen, Gaoyu and Zhang, Xiaoqun and Huang, Qiu and Ji, Hui and Gao, Hao},
  journal={Physics in Medicine \& Biology},
  volume={65},
  number={12},
  pages={125009},
  year={2020},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{chen2020airnet,
  title={AirNet: fused analytical and iterative reconstruction with deep neural network regularization for sparse-data CT},
  author={Chen, Gaoyu and Hong, Xiang and Ding, Qiaoqiao and Zhang, Yi and Chen, Hu and Fu, Shujun and Zhao, Yunsong and Zhang, Xiaoqun and Ji, Hui and Wang, Ge and others},
  journal={Medical physics},
  volume={47},
  number={7},
  pages={2916--2930},
  year={2020},
  publisher={Wiley Online Library},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhao2023ae,
  title={AE-FLOW: Autoencoders with normalizing flows for medical images anomaly detection},
  author={Zhao, Yuzhong and Ding, Qiaoqiao and Zhang, Xiaoqun},
  journal={International Conference on Learning Representations (<b>ICLR</b>)},
  year={2023},
  abbr={Conference},
  selected={true},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{he2022low,
  title={Low-rank and framelet based sparsity decomposition for interventional MRI reconstruction},
  author={He, Zhao and Zhu, Ya-Nan and Qiu, Suhao and Wang, Tao and Zhang, Chencheng and Sun, Bomin and Zhang, Xiaoqun and Feng, Yuan},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={69},
  number={7},
  pages={2294--2304},
  year={2022},
  publisher={IEEE},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{ding2022dataset,
  title={A dataset-free deep learning method for low-dose CT image reconstruction},
  author={Ding, Qiaoqiao and Ji, Hui and Quan, Yuhui and Zhang, Xiaoqun},
  journal={Inverse Problems},
  volume={38},
  number={10},
  pages={104003},
  year={2022},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{ding2022mri,
  title={Mri reconstruction by completing under-sampled k-space data with learnable fourier interpolation},
  author={Ding, Qiaoqiao and Zhang, Xiaoqun},
  journal={International Conference on Medical Image Computing and Computer-Assisted Intervention(<b>MICCAI</b>)},
  pages={676--685},
  year={2022},
  organization={Springer Nature Switzerland Cham},
  abbr={Conference},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{ding2021learnable,
  title={Learnable multi-scale fourier interpolation for sparse view ct image reconstruction},
  author={Ding, Qiaoqiao and Ji, Hui and Gao, Hao and Zhang, Xiaoqun},
  journal={Medical Image Computing and Computer Assisted Intervention (<b>MICCAI</b>)},
  pages={286--295},
  year={2021},
  organization={Springer},
  abbr={Conference},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{hu2023sea,
  title={Sea-net: Structure-enhanced attention network for limited-angle cbct reconstruction of clinical projection data},
  author={Hu, Dianlin and Zhang, Yikun and Li, Wangyao and Zhang, Weijie and Reddy, Krishna and Ding, Qiaoqiao and Zhang, Xiaoqun and Chen, Yang and Gao, Hao},
  journal={IEEE Transactions on Instrumentation and Measurement},
  year={2023},
  publisher={IEEE},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={learning},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{liu2023binary,
  title={Binary quantized network training with sharpness-aware minimization},
  author={Liu, Ren and Bian, Fengmiao and Zhang, Xiaoqun},
  journal={Journal of Scientific Computing},
  volume={94},
  number={1},
  pages={16},
  year={2023},
  publisher={Springer US New York},
  abbr={Journal},
  cate_primary={learning},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhu2021stochastic,
  title={A stochastic variance reduced primal dual fixed point method for linearly constrained separable optimization},
  author={Zhu, Ya-Nan and Zhang, Xiaoqun},
  journal={SIAM Journal on Imaging Sciences},
  volume={14},
  number={3},
  pages={1326--1353},
  year={2021},
  publisher={SIAM},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={imaging},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{xiong2023pet,
  title={PET-3DFlow: A Normalizing Flow Based Method for 3D PET Anomaly Detection},
  author={Xiong, Zhe and Ding, Qiaoqiao and Zhao, Yuzhong and Zhang, Xiaoqun},
  journal={International Workshop on Computational Mathematics Modeling in Cancer Analysis},
  pages={91--100},
  year={2023},
  organization={Springer},
  abbr={Conference},
  cate_primary={learning},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{bian2021three,
  title={A three-operator splitting algorithm for nonconvex sparsity regularization},
  author={Bian, Fengmiao and Zhang, Xiaoqun},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={4},
  pages={A2809--A2839},
  year={2021},
  publisher={SIAM},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}


@article{hamilton2016hybrid,
  title={A hybrid segmentation and d-bar method for electrical impedance tomography},
  author={Hamilton, Sarah J and Reyes, Juan Manuel and Siltanen, Samuli and Zhang, Xiaoqun},
  journal={SIAM Journal on Imaging Sciences},
  volume={9},
  number={2},
  pages={770--793},
  year={2016},
  publisher={Society for Industrial and Applied Mathematics},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{zhang2021variable,
  title={Variable metric extrapolation proximal iterative hard thresholding method for $$\backslash$ell\_0 $ minimization problem},
  author={Zhang, Xue and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:2108.03365},
  year={2021},
  abbr={Preprint},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}


@article{ding2017image,
  title={Image-domain multi-material decomposition for dual-energy ct based on correlation and sparsity of material images},
  author={Ding, Qiaoqiao and Niu, Tianye and Zhang, Xiaoqun and Long, Yong},
  journal={arXiv preprint arXiv:1710.07028},
  year={2017},
  abbr={Preprint},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}


@article{zhang2020variational,
  title={Variational bimodal image fusion with data-driven tight frame},
  author={Zhang, Ying and Zhang, Xiaoqun},
  journal={Information Fusion},
  volume={55},
  pages={164--172},
  year={2020},
  publisher={Elsevier},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{liu20155d,
  title={5D respiratory motion model based image reconstruction algorithm for 4D cone-beam computed tomography},
  author={Liu, Jiulong and Zhang, Xue and Zhang, Xiaoqun and Zhao, Hongkai and Gao, Yu and Thomas, David and Low, Daniel A and Gao, Hao},
  journal={Inverse Problems},
  volume={31},
  number={11},
  pages={115007},
  year={2015},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={true},
}

@article{bian2021parameterized,
  title={A parameterized douglas--rachford splitting algorithm for nonconvex optimization},
  author={Bian, Fengmiao and Zhang, Xiaoqun},
  journal={Applied Mathematics and Computation},
  volume={410},
  pages={126425},
  year={2021},
  publisher={Elsevier},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{zhang2017accelerated,
  title={An accelerated proximal iterative hard thresholding method for $\ell_0$ minimization},
  author={Zhang, Xue and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:1709.01668},
  year={2017},
  abbr={Preprint},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{zhang2021image,
  title={Image fusion network for dual-modal restoration},
  author={Zhang, Ying and Ren, Xuhua and Clifford, Bryan Alexander and Wang, Qian and Zhang, Xiaoqun},
  journal={Inverse Problems and Imaging},
  volume={15},
  number={6},
  pages={1409--1419},
  year={2021},
  publisher={Inverse Problems and Imaging},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{lv2019greedy,
  title={A greedy algorithm for sparse precision matrix approximation},
  author={Lv, Didi and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:1907.00723},
  year={2019},
  abbr={Preprint},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{ding2018image,
  title={Image-domain multi-material decomposition for dual-energy CT via total nuclear norm and $\ell_0$ norm},
  author={Ding, Qiaoqiao and Niu, Tianye and Zhang, Xiaoqun and Long, Yong},
  journal={Med Phys},
  year={2018},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{xiong2023symot,
  title={SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy},
  author={Xiong, Zhe and Ding, Qiaoqiao and Zhang, Xiaoqun},
  journal={arXiv preprint arXiv:2308.13815},
  year={2023},
  abbr={Preprint},
  cate_primary={learning},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}

@article{zhang2014forward,
  title={Forward--backward splitting method for quantitative photoacoustic tomography},
  author={Zhang, Xue and Zhou, Weifeng and Zhang, Xiaoqun and Gao, Hao},
  journal={Inverse problems},
  volume={30},
  number={12},
  pages={125012},
  year={2014},
  publisher={IOP Publishing},
  abbr={Journal},
  cate_primary={imaging},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}



@article{chen2013primal,
  title={A primal-dual fixed point algorithm based on proximity operator for convex set constrained separable problem},
  author={Chen, Peijun and Huang, Jianguo and Zhang, Xiaoqun},
  journal={J. Nanjing Norm. Univ. Nat. Sci. Ed},
  volume={36},
  number={3},
  pages={1--5},
  year={2013},
  abbr={Journal},
  cate_primary={optimization},
  cate_secondary={none},
  selected={false},
  bibtex_show={true},
  selected={false},
}